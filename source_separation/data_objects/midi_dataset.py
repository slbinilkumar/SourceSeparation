from source_separation.data_objects.music import Music
from pathos.threading import ThreadPool
from pathlib import Path
from random import shuffle
from typing import List
import numpy as np
from time import perf_counter as timer


class MidiDataset:
    def __init__(self, root: Path, is_train: bool, hparams):
        """
        Creates a dataset that synthesizes instrument tracks from midi files. Call 
        MidiDataset.generate() to iterate over the dataset and retrieve pairs of fixed-size 
        segments (called chunks) from the generated waveforms, with only the instruments
        selected playing.
        
        :param root: path to the directory containing midi files and the corresponding index 
        files generated from parse_dataset.py.
        :param is_train: if True, the train index file is used. Otherwise, the test index file 
        will be used. 
        """
        self.hparams = hparams
        
        # Build the index: a list of tuples (fpath, instruments) 
        index_fname = "midi_%s_index.txt" % ("train" if is_train else "test")
        index_fpath = root.joinpath(index_fname)
        with index_fpath.open("r") as index_file:
            index = [line.split(":") for line in index_file]
        self.index = [(root.joinpath(fpath.replace('"', '')),
                      list(map(int, instruments.split(',')))) for fpath, instruments in index]

    def generate(self, source_instruments: List[int], target_instruments: List[int],
                 batch_size: int, n_threads, music_buffer_size):
        # Todo: redo the doc
        # """
        # :param chunk_duration: the duration, in seconds, of the audio segments that the dataset 
        # yields.
        # :param source_instruments: a list of instrument IDs. Only musics containing all these 
        # instruments will be sampled from, and only the tracks for these instruments will be 
        # generated for the first output of the dataset.
        # :param target_instruments: a list of instrument IDs. All instruments must also be 
        # contained in <source_instruments>. Only the tracks for these instruments will be 
        # generated for the second output of the dataset.
        # :param batch_size: batch size for the data generated by the dataset.
        # :param n_threads: number of threads used for synthesizing the midi files.
        # """
        # Todo: later, think about how the instruments should be selected
        assert all((i in source_instruments) for i in target_instruments), \
            "Some target instruments are not in the set of the source instruments."
        
        # Create a generator that loops infinitely over the songs in a random order
        def midi_fpath_generator():
            midi_fpaths = list(self._get_files_by_instruments(source_instruments))
            while True:
                shuffle(midi_fpaths)
                yield from midi_fpaths
        midi_fpath_generator = midi_fpath_generator()
                
        # Define a function to fill a buffer
        def begin_next_buffer():
            func = lambda fpath: self._extract_chunks(fpath, source_instruments, target_instruments)
            midi_fpaths = [next(midi_fpath_generator) for _ in range(music_buffer_size)]
            return pool.uimap(func, midi_fpaths)
        
        # Create the threadpool and initialize the buffers
        pool = ThreadPool(n_threads)
        buffer = []
        next_buffer = begin_next_buffer()
        
        while True:
            while len(buffer) < batch_size:
                # Retrieve the elements from the next buffer that were generated in the background.
                # If it is not done generating, block until so with a call to list().
                print("Generating a new buffer from %d musics... " % music_buffer_size, end="")
                start = timer()
                next_buffer = list(next_buffer)
                delta = timer() - start
                print("Done! Blocked %dms to generate the buffer." % (int(delta * 1000)))
                
                # Flatten the buffer to retrieve a list of chunks
                next_buffer = [chunk for chunks in next_buffer for chunk in chunks]
                print("Generated %d new chunks." % len(next_buffer))
                
                # Shuffle the buffer so as to mix different musics in a same batch
                shuffle(next_buffer)
                
                # Append all the contents of the new buffer to the current buffer, so that it now
                # has more elements to generate new batches
                buffer.extend(next_buffer)
                
                # Begin a new buffer in the background
                next_buffer = begin_next_buffer()
            
            # Consume elements from the buffer to generate a batch
            batch = buffer[:batch_size]
            yield np.array(batch).transpose((1, 0, 2))
            del buffer[:batch_size]

    def _get_files_by_instruments(self, instruments, mode="and"):
        """
        Yields midi filepaths in the dataset that contain a specific set of instruments.

        :param instruments: a list of instrument IDs.
        :param mode: if "and", the midi file will only be returned if all the instruments 
        required are found in the midi. If "or", it will be returned as soon as one instrument is 
        found in the midi.  
        :return: a generator that yields the midi filepaths as strings
        """
        for midi_fpath, midi_instruments in self.index:
            selector = all if mode == "and" else any
            if selector((i in midi_instruments) for i in instruments):
                yield midi_fpath
                
    def _debug_compare_chunks(self, source_chunk, target_chunk):
        zero_prop = lambda chunk: np.sum(np.abs(chunk) < self.hparams.silence_threshold) \
                                  / len(chunk)
        zero_source_prop = zero_prop(source_chunk)
        zero_target_prop = zero_prop(target_chunk)
        similarity_prop = zero_prop(source_chunk - target_chunk)
        np.sum(np.abs(source_chunk - target_chunk) < self.hparams.silence_threshold) /\
            len(target_chunk)
        print("Proportion in the source that is silence: %.2f%%" % (zero_source_prop * 100))
        print("Proportion in the target that is silence: %.2f%%" % (zero_target_prop * 100))
        print("Proportion of the chunk that is equal: %.2f%%" % (similarity_prop * 100))
    
    def _extract_chunks(self, midi_fpath, source_instruments, target_instruments):
        # Load the midi file from disk
        music = Music(sample_rate=self.hparams.sample_rate, fpath=str(midi_fpath))
        
        # Ignore songs that are too long (prevents corrupted midis from blocking the sampling)
        if music.mid.length > self.hparams.max_midi_duration:
            return np.array([]), np.array([]) 

        # Generate a waveform for the reference (source) audio and the target audio the network
        # had to produce.
        source_wav = music.generate_waveform(source_instruments)
        target_wav = music.generate_waveform(target_instruments)
        assert len(source_wav) == len(target_wav)
        
        # Pad waveforms to a multiple of the chunk size
        chunk_size = self.hparams.chunk_duration * self.hparams.sample_rate
        padding = chunk_size - (len(source_wav) % chunk_size)
        source_wav = np.append(source_wav, np.zeros(padding))
        target_wav = np.append(target_wav, np.zeros(padding))
        
        # Iterate over the waveforms to create chunks
        chunks = []
        for i in range(0, len(source_wav), chunk_size):
            source_chunk, target_chunk = source_wav[i:i + chunk_size], target_wav[i:i + chunk_size]
            
            # Compute what proportion of the waveform is repeated without change in the target 
            # waveform. 
            abs_diff = np.abs(source_chunk - target_chunk)
            equal_prop = np.sum(abs_diff < self.hparams.silence_threshold) / len(abs_diff)
            if equal_prop >= self.hparams.chunk_equal_prop_max:
                continue
                
            # Compute what proportion of the target waveform is silence.
            silence_prop = np.sum(np.abs(target_chunk) < self.hparams.silence_threshold) \
                           / len(target_chunk)
            if silence_prop >= self.hparams.chunk_silence_prop_max:
                continue
                
            # Accept samples that were not discarded
            chunks.append((source_chunk, target_chunk))
            
        return chunks
